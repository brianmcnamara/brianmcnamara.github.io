---
title: "Machine Learning Classification of Adults with  Autism Spectrum Disorder"
author: Brian McNamra, Camila Lora, Donyoung Yang, Fabiana Flores, Paul Daly
date: "April 29, 2018"
output:
  prettydoc::html_pretty:
    highlight: github
    theme: cayman
  html_document:
    df_print: paged
---

### **CONTENTS**
***

* [1. Abstract](#abstract)
* [2. Introduction](#introduction)
* [3. Classification Algorithms](#classification-algorithms)
    + [3.1. Decision Tree](#decision-tree)
    + [3.2. Random Forest](#random-forest)
* [4. The Data-set](#the-data-set)
* [5. Data Exploration](#data-exploration)
* [6. Data Cleaning](#data-cleaning)
    + [6.1. Missing Values](#missing-values)
    + [6.2. Outlier Values](#outlier-values)
    + [6.3. Duplicates](#duplicates)
* [7. Data Visualisation](#data-visualisation)
    + [7.1. Boxplots - Age](#boxplots-age)
    + [7.2. Boxplots - Result](#boxplots-result)
    + [7.3. Bar Chart - Ethnicity](#bar-chart-ethnicity)
    + [7.4. Bar Charts - Jaundice / Relative](#bar-charts-jaundice-relative)
* [8. Data Pre-Processing](#data-pre-processing)
    + [8.1. Variable Reduction](#variable-reduction)
    + [8.2. Partitioning](#partitioning)
* [9. Concepts & Techniques](#concepts-techniques)
    + [9.1 Overfitting Vs. Underfitting](#overfitting-vs-underfitting)
    + [9.2. K-Fold Cross Validation](#k-fold-cross-validation)
    + [9.3. Confusion Matrix](#confusion-matrix)
    + [9.4. Roc Curve](#roc-curve)
* [10. Results](#results)
    + [10.1. Decision Tree Results](#decision-tree-results)
    + [10.2. Random Forest Results](#random-forest-results)
* [11. Conclusion](#conclusion)
* [12. References](#references)

```{r setup, include = FALSE}
knitr::opts_chunk$set(comment = "")
```


```{r include = FALSE}
# Load packages
library(prettydoc)
library(tidyverse)
library(skimr)
library(foreign)
library(varhandle)
library(ggplot2)
library(ggpubr)
library(pander)
library(knitr)
library(kableExtra)
library(rpart)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(randomForest)
library(caret)
library(caTools)
library(pROC)
```


## **1. ABSTRACT** {#abstract}
***

The purpose of this study is to evaluate the accuracy of two machine learning classification
algorithms, Decision Tree and Random Forest, when attempting to classify individuals who may be likely to suffer from Autism Spectrum Disorder.

This study details the data cleaning and pre-processing steps taken to prepare the data-set for classification such as dealing with missing values, outlier removal, variable selection, and the partitioning of the data into training and testing subsets. It also walks through the validation and evaluation methods used when choosing the most appropriate models for each algorithm.

Evaluation of the prediction accuracy of the two classification algorithms used indicates that while both algorithms predicted the binary target variable with high level of accuracy, the Random Forest model was significantly more accurate than the Decision Tree model.

The authors of this paper have concluded the Random Forest algorithm is the best choice when predicting whether an individual is likely to have ASD or not. However, it is noted that both algorithms were far more efficient at predicting a negative response to the target variable than they were at predicting a positive response. This may be down to the proportion of positive responses the models had to train with. A larger, more balanced training data-set may increase the model's accuracy further when predicting on new data.


## **2. INTRODUCTION** {#introduction}
***

In recent times, the application of Machine Learning to cross-disciplinary subjects have been very active and successful, especially in the fields of biology and neurology. Many researchers are interested in creating computational frameworks for automatically generating patterns and trends in large medical datasets. A learned data representation can help visualise data to assist humans in clinical decision making and predict a target variable from a set of input features. (Daniel Bone, Matthew S. Goodwin, Matthew P. Black, Chi-Chun Lee, Kartik Audhkhasi, Chrikanth Narayanan, 2014)

The data selected for this project is the Autistic Spectrum Disorder (ASD) screening data for adults. ASD refers to several related disorders that normally begin in childhood and continue in adulthood. There is no cure for ASD, but treatments can help to improve symptoms. As per HSE (2017), the symptoms can include:

* Social interaction where it is difficult to understand situations and other people's feelings and emotions.

* Difficulties to communicate, which can involve delayed language development, also not being able to take part in           conversations properly.

* Unusual physical behaviour such as doing repetitive physical movements, which becomes a routine, then the behaviour        becomes routine and the individual can get upset if the routine is disrupted.

The ASD symptoms can vary from person to person, and it can classify in three main types. The most typical type is "autistic disorder", followed by "Asperger syndrome" and "pervasive developmental disorder" (PDD). The third one is also known as 'atypical autism'. ASD are estimated to affect 1 in every 100 children and boys are more likely to develop ASD than girls by four times. (HSD, 2017)

Researches and studies about classification on data related Autism have been conducted mainly by clinical experts and data scientists. A few ASD studies have analysed functional connectivity MRI(fcMRI) scan data to classify whether a dataset is coming from ASD or a typically developing participant solely based on functional connectivity. (Collen P. Chen, Christopher L. Keown, Afroos Jahedi, Aarti Nair, Mark E. Pflieger, Barbara A. Bailey, Ralph-Axel Muller, 2015).

Another stream is to develop diagnostic machine learning algorithms using human behaviour data. The Autism Diagnostic Interview-Revised (Lord, C., Risi, S., Lambrecht, L., Cook, E. H, Jr, Leventhal, B. L., DiLavore, P. C., et al., 2000) and the Autism Diagnostic Observation Schedule (Gotham, K., Risi, S., Pickles, A., & Lord, C., 2007) proved a certain level of usefulness of objective machine learning methods for the diagnosis of autism.


## **3. CLASSIFICATION ALGORITHMS** {#classification-algorithms}
***

Classification is a technique to predict what group certain instance is going to be. To create classifiers, we use from the given learning data set and evaluate on the test samples, so it is possible predict what class the group is following to. For the Ian Witten and Frank Eibe (2017) "classification is sometimes called supervised because, in a sense, the scheme operates under supervision by being provided with the actual outcome for each of the examples." 


### **3.1. DECISION TREE** {#decision-tree}

The decision tree is a visual representation that is used as part of a selection criteria, or even to support the selection of specific data, considering the overall structure. It represents choices and its results in the form of a tree. It can start with simple questions that will have 2 or more answers, leading to a further question, and so on. It will support to identify and classify the data. Decision trees are mostly used in Data Mining applications using R and Machine Learning. (Brown, 2012)

Below example show the decision tree where an incoming error condition can be classified.

<center>

![fig 3.1. - Decision Tree (Brown, 2012)](D:\Brian\OneDrive - National College of Ireland\EDUCATION\DATA ANALYTICS\DATA AND WEB MINING\CA\CA 2\ASD Classification\D-Tree.gif)

</center>

A decision tree will divide the data into leaf nodes and each one of them will represent an attribute. In a nutshell, decision tree is a splitting method that is applied to demonstrate every possible outcome of a decision. (Jain, 2016)

The name decision tree already implies the meaning of the technique. From root to leaves it can predict and classify outcomes, leading to a new question. The author sustain that the tree is placed upside down, with the leaves indicating the outcomes, and the root at the top, which represents the original dataset. Zangh (2016), affirms the following: "Because the parent population can be split into in numerous patterns, we are interested in the one with the greatest purity. In technical terminology, purity can be described by entropy."

To control the size and to select the optimal tree size the complexity parameter (cp) is used. It will stop the tree construction in case a new variable need to be added and its value is above the cp. It stipulates how the cost of a tree is penalized considering the number of terminal nodes. (Williams, 2010)


### **3.2. RANDOM FOREST** {#random-forest}

The Random forest algorithm is a supervised classification algorithm. As the name suggests, this algorithm creates the forest which in turn develops large numbers of random decision trees analysing sets of variables.

Each decision tree in the forest considers a random subset of features when forming questions and only has access to a random set of the training data points. This increases diversity in the forest leading to more robust overall predictions and the name 'random forest.'

Random Forest works by splitting the data set into three parts:

* Training
* Testing
* Cross validation


#### **Training**

The training process creates a separate data set. The use of a series of questions is part of the training. The questions are used to reduce the range of data until we come up with a prediction.  

A very simple query like predicting tomorrow's maximum temperature would need to work through an entire series of queries (e.g. what season it is, maximum temperature of season, lowest temperature, yesterday temperature, average season temperature etc) to answer this query. 

Humans usually ask questions which could have multiple answers.  This is not the case for decision tree implemented in machine learning.  This will list all possible alternatives to every question and will answer all questions in True/False form. This concept is different and can be tough to grasp because it is not how humans naturally think.  The number of queries have a diminishing effect and need to be relevant to your data set.  This also leads to the tree effect.

The model has no prior knowledge of the data and will learn everything from the data provided to it.  The model learns from the data provided to it and can also use historic data. 

The fundamental idea behind a random forest is to combine many decision trees into a single model. Individually, predictions made by decision trees (or humans) may not be accurate, but combined, the predictions will be closer to the mark on average.


#### **Testing**

The test data is then tested against the trained data. Each Random Forest will predict different values form the same values. This concept of voting is known as majority voting.  


#### **Cross Validation**

Cross-validation is a technique used to protect against overfitting in a predictive mode especially in cases where the amount of data may be limited. Cross-validation is a technique used to evaluate predictive models.  This is done by partitioning the original sample into a training set to train the model, and a test set to evaluate it. In k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples. This process is sometimes called rotation estimation. 

In cross-validation, you make a fixed number of folds (or partitions) of the data, run the analysis on each fold, and then average the overall error estimate.

The Random Forest algorithm has three concepts:

* Bootstrap
* Bagging
* Decision Trees


#### **Bootstrap**

They involve repeatedly drawing samples from a training set and re???tting a model of interest on each sample to obtain additional information about the ???tted model.

There are two types of statistic, descriptive and inferential.  Bootstrapping uses inferential which are produced through complex mathematical calculations that allow scientists to infer trends about a larger population based on a study of a sample taken from it. 

Inferential statistics start with a sample and then generalizes to a population. This information about a population is not stated as a number. Instead, scientists express these parameters as a range of potential numbers, along with a degree of confidence.

The bootstrap samples with replacement from the sample. As there is replacement the bootstrap will more than likely not be the same twice. The bootstrapping allows some data to be duplicated, other data to be omitted. Computers can create thousands of bootstrap samples in a relatively short time. This has been around since 1979 paper by Bradley Efron and increased in popularity as computer power increased and cost reduced.


#### **Bagging**

This is used to improve the performance of a predictive model. It does this by taking multiple sampling by using replacement within the sample from a training set. This method would be more useful (when the predictors are not stable) when the random sample from the training set is very different. The greater variability should hopefully lead to greater chance of better results. When the samples are very similar the use of Bagging is superfluous.


#### **MTRY**

In Random Forest the Mtry is the number of variables available for splitting at each tree node, the default value of this parameter depends on which R package is used to fit the model.

Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging) to sub-sample data samples used for training. OOB is the mean prediction error on each training sample x???, using only the trees that did not have x??? in their bootstrap sample

The random selection of variables at each node splitting step is what makes it a random forest, as opposed to just a bagged estimator. Quoting from The Elements of Statistical Learning, p 588 in the second edition. Reducing mtry (Number of random variables used in each tree) reduces both the correlation and the strength and therefore increasing it increases both.

Somewhere in between is an "optimal" range of mtry - usually quite wide. Using this approach Out-of-bag (OOB) error rate a value of mtry in the range can quickly be found.  This is the only adjustable parameter to which random forests is somewhat sensitive.

<center>

![fig 3.2. - Illustration of a Random Forest](D:\Brian\OneDrive - National College of Ireland\EDUCATION\DATA ANALYTICS\DATA AND WEB MINING\CA\CA 2\ASD Classification\R-Forest.png)

</center>


## **4. THE DATA-SET** {#the-data-set}
***

Our chosen data-set, entitled, ["Autism Screening Adult Data Set"](http://archive.ics.uci.edu/ml/datasets/Autism+Screening+Adult#), is an open source data-set available from The [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php). It was donated to the repository on the 24th of December 2017 by Dr.Fadi Fayez Thabtah (fadi.fayez@manukau.ac.nz ), Department of Digital Technology, Manukau Institute of Technology, Auckland, New Zealand.

The data-set consists of 704 observations of 21 variables. There are just two numeric variables present (age and result), with the remaining variables being categorical and binary in nature. The data describes ASD screening results, some of which appear to have been harvested from an app developed by Dr.Fadi Fayez named "ASDQuiz", which is available for both [Android](https://play.google.com/store/apps/details?id=com.asd.asdquiz&hl=en) and [iOS](https://itunes.apple.com/us/app/asd-and-autism-tests/id1279149795?ls=1&mt=8) devices.

The raw data-set contains ten binary variables representing the screening questions (`A1_Score` to `A10_Score`), as well as the categorical variables of `gender`, `ethnicity`, `jaundice`, `autism`, `country_of_res`, `used_app_before`, `age_desc`, `relation` and `Class/ASD`. There are also two numeric variables named `age` and `result`.

Please see the official description of the variables below. Note, we have included the actual questions associated with the A1_Score to A10_Score variables. The questions ask for a simple binary answer of either agree or disagree.


```{r echo = FALSE}
varTable <- data.frame(
  Variables <- c("Age:",
            "Gender:",
            "Ethnicity:",
            "Born with Jaundice:",
            "Family member with PDD:",
            "Who is completing the test:",
            "Country of Residence:",
            "Used the screening app before:",
            "Screening Method Type:",
            "Question 1 Answer:",
            "Question 2 Answer:",
            "Question 3 Answer:",
            "Question 4 Answer:",
            "Question 5 Answer:",
            "Question 6 Answer:",
            "Question 7 Answer:",
            "Question 8 Answer:",
            "Question 9 Answer:",
            "Question 10 Answer:",
            "Screening Score:"),
  Description <- c("Age in years",
            "Male or female",
            "List of common ethnicities in text format",
            "Whether case was born with jaundice",
            "Whether any immediate family member has a PDD",
            "Parent, self, caregiver, medical sta???, clinician, etc",
            "List of countries in text format",
            "Whether the user has used screening app",
            "Type of screening method chosen based on age category",
            "I often notice small sounds when others do not",
            "I usually concentrate more on the whole picture, rather than the small details",
            "I find it easy to do more than one thing at once",
            "If there is an interruption, I can switch back to what I was doing very quickly",
            "I find it easy to read between the lines when someone is talking to me",
            "I know how to tell if someone listening to me is getting bored",
            "When I'm reading a story I find it difficult to work out the character's intentions",
            "I like to collect information about categories of things (e.g. types of cars, types of bird, types of train, types of plant, etc)",
            "I find it easy to work out what someone is thinking or feeling just by looking at their face",
            "I find it difficult to work out peoples intentions",
            "Final score obtained based on scoring algorithm of screening method used"))

names(varTable) <- c("Variables", "Description")

emphasize.strong.cols(1)

pander(varTable, justify = "left", style = "grid")

# # Function for formatting table cells
# format_cells <- function(df, rows ,cols, value = c("italics", "bold", "strikethrough")){
# 
#   # select the correct markup
#   map <- setNames(c("*", "**", "~~"), c("italics", "bold", "strikethrough"))
#   markup <- map[value]  
# 
#   for (r in rows){
#     for(c in cols){
# 
#       # Make sure values are not factors
#       df[[c]] <- as.character( df[[c]])
# 
#       # Update formatting
#       df[r, c] <- paste0(markup, df[r, c], markup)
#     }
#   }
# 
#   return(df)
# }
# 
# # Format the Variables column with bold text
# varTable %>%
#   format_cells(1:20, 1, "bold") %>%
#   kable("html")
```


## **5. DATA EXPLORATION** {#data-exploration}
***

```{r include = FALSE}
# Import the dataset
dataASD <- read.arff("D:/Brian/OneDrive - National College of Ireland/EDUCATION/DATA ANALYTICS/DATA AND WEB MINING/CA/CA 2/ASD Classification/Autism-Adult-Data Plus Description File/Autism-Adult-Data.arff")
```

The first step in the analysis of the raw data-set is to explore. By doing this we may discover changes that need to be made that will make the data-set easier to work with and could potentially increase the accuracy of our results.

Below you can see the first five rows of the data-set. This gives a good overview of what a typical observation from the data looks like.

```{r echo = FALSE}
# Display first five rows of the dataset
pander(head(dataASD, 5))
```

We can also check the structure of the data-set. Here we can see the levels of our categorical variables. From this we can see that there are some spelling errors in the variable names (`jundice`, `austim`, `contry_of_res`), and that the `Class/ASD` variable will cause trouble due to the illegal forward slash character.  

The `age_desc` variable appears to have just one level, making it a likely candidate to be dropped from the data set. Also, the `contry_of_res` variable has 67 levels, which is too many for the Random Forest algorithm to handle, so this can probably be dropped as well.

```{r echo = FALSE}
# Display the structure of the dataset. ### NOTE ### - 19 categorical variables (factors), 2 numeric variables
str(dataASD)
```


Before proceeding any further with the exploration, we will first fix some of the variable names to aid in readability.  

Here, we have truncated the variable names of the `A1_Score` to `A10_Score` variables to `A1` to `A10`. Also, we have corrected the spelling errors/illegal characters in the `austim`, `jundice`, `contry_of_res` and `Class/ASD` variables.

```{r echo = FALSE}
# Truncate names of the answer variables to aid readability
names(dataASD) <- gsub("_Score", "", names(dataASD), fixed = TRUE)

# Correct incorrect/illegal spelling of variable names
dataASD <- dplyr::rename(dataASD, autism = 'austim')
dataASD <- dplyr::rename(dataASD, jaundice = 'jundice')
dataASD <- dplyr::rename(dataASD, country = 'contry_of_res')
dataASD <- dplyr::rename(dataASD, Class_ASD = 'Class/ASD')
```


Let's have a look at the corrected variable names.

```{r echo = FALSE}
# Display variable names
names(dataASD)
```


Let's also check the levels of our categorical variables. While they are mostly binary in nature, the `relation`, `ethnicity` and `country` variables have more than 2 levels. We will be dropping the `country` variable later, so let's have a look at `relation`.

```{r echo = FALSE} 
# Display levels of the relation variable
levels(dataASD$relation)
```


And `ethnicity`. There appears to be a duplicate category of `others` which will need to be corrected.

```{r echo = FALSE}
# Display levels of the ethnicity variable
levels(dataASD$ethnicity)
```


As we will be attempting to classify whether or not an individual may have ASD, it would be useful to know the proportion of the data-set that has been classed YES or NO. Here, we can see that 26.85% of the observations present have been classed as potentially having ASD.

```{r echo = FALSE, warning = FALSE}
# Calculate percentages of Class_ASD variable
pct <- dataASD %>%
  group_by(Class_ASD) %>%
  summarise(count = n() / nrow(.) )

pander(pct)
```


By using the `summary` function, we can see some basic descriptive statistics for the variables. Here, we can see that we have a number of missing values in the `ethnicity`, `relation` and `age` variables. Also, there appears to be an impossibly large Max. value present in the `age` variable (383), possibly due to a typing error.

```{r echo = FALSE}
# Basic statistics of the dataset
panderOptions('missing', '')
pander(summary(dataASD), justify = "left")
```


## **6. DATA CLEANING** {#data-cleaning}
***

Now that we have we transformed the data-set to a much more easily readable state, we can begin to clean the data of unwanted information to prepare it for use with our chosen machine learning algorithms.

### **6.1. MISSING VALUES** {#missing-values}

The output below displays a count of the observations with missing values for each variable. There are 95 each for `ethnicity` and `relation`, and 2 for `age`.

```{r echo = FALSE}
# Check for NA values in each variable
t <- colSums(is.na(dataASD))

# emphasize.strong.cols(11)
# emphasize.strong.cols(13)
emphasize.strong.cols(c(11, 13, 20))

pander(t)
```


The missing values are predominantly categorical. This makes it difficult to create replacement values as we cannot substitute the mean or median for non numeric variables.

The best course of action is to remove all of the observations containing missing values. This is done by creating a copy of our data frame and using the na.omit function to strip out all of the NAs.

The output below shows a count of missing values on the data frame and confirms that there are no longer any NAs present.


```{r echo = FALSE}
# Drop all observations containing NA values. Creates a new data frame, omitting the NA values
# ### NOTE ### The NA values were mostly categorical, making it difficult to impute replacement values
dataASDclean <- na.omit(dataASD)
pander(sapply(dataASDclean, function(x) sum(is.na(x))))
```


Counting the observations present in the new data frame without missing values reveals that there are now 609 observations, confirming that we have indeed dropped 95 observations.

```{r echo = FALSE}
# Count of observations
pander(count(dataASDclean))
```


### **6.2. OUTLIER VALUES** {#outlier-values}

You may recall that we had discovered an impossibly large Max. value of 383 in the `age` variable. Given that there are already many typographical errors present in the data-set, it is reasonable to assume that this too is the result of a typing error and the intended value was 38.

The code below replaces the value of 383 with 38. Looking at the `summary` statistics now, we can see that the Max. age is now 64, which is much more in line with what is expected from this data-set.

```{r}
# Fix outlier in age column. Replace value of 383 with 38
dataASDclean$age[dataASDclean$age == 383] <- 38
pander(summary(dataASDclean$age))
```


### **6.3. DUPLICATES** {#duplicates}

There is also a duplicated category in the `ethnicity` variable. The categories of `Others` and `others` are obviously intended to be one value.

The code below amalgamates both values into just one level of the `ethnicity` factor.

```{r}
# Fix duplicated "others" category in the ethnicity variable
levels(dataASDclean$ethnicity) <- gsub("others", "Others", levels(dataASDclean$ethnicity))
levels(dataASDclean$ethnicity)
```


## **7. DATA VISUALISATION** {#data-visualisation}
***
With the missing values, outlier values and duplicates taken care of, we can now start to visually explore the data that we will be working with.


### **7.1. BOXPLOTS - AGE** {#boxplots-age}

The boxplot below represents the age distribution of males and females and their class, ASD YES/ASD NO. We can see that age ranges from the late teens to the early to mid 60s for both genders. Males appear to have a wider distribution of individuals classed as having ASD.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 7.1. - Boxplots - Age*"}
# Boxplots showing the age distribution for both genders
ggplot(dataASDclean, aes(x = gender, y = age, fill = Class_ASD)) +
  geom_boxplot(color = "black", alpha = 0.5) +
  theme_pubr() +
  fill_palette("jco")
```


### **7.2. BOXPLOTS - RESULT** {#boxplots-result}

This boxplot represents the distribution of the screening result scores for both males and females and their class, ASD YES/ASD NO. There is no difference in the distribution between males and females, which indicates that the screening criteria treats both genders equally. We can also see that a score of about 7 results in an individual being classed as having ASD.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 7.2. - Boxplots - Result*"}
# Boxplots showing the result distribution for both genders
ggplot(dataASDclean, aes(x = gender, y = result, fill = Class_ASD)) +
  geom_boxplot(color = "black", alpha = 0.5) +
  theme_pubr() +
  fill_palette("jco")
```


Querying the data with the code below confirms that a score of 7 or more results in an individual being classed as having ASD. The code simply returns a count of results that are 7 or more, and a count of the observations with `Class_ASD` = YES. As you can see, they are exactly the same.

```{r}
# Confirm that a result of 7 or more is classified as ASD
c1 <- dataASDclean %>% count(result >= 7)

c2 <- dataASDclean %>% count(Class_ASD == "YES")

comp_c1_c2 <- cbind(c1,c2)

pander(comp_c1_c2)
```


### **7.3. BAR CHART - ETHNICITY** {#bar-chart-ethnicity}

This bar chart represents the proportions of each ethnicity present in the data. White-Europeans account for approximately one third of the data, followed by Asians and Middle Eastern people.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 7.3. - Bar Chart - Ethnicity*"}
# Stacked proportions of the ethnicities present in the dataset
pPlot <- dataASDclean %>% count(ethnicity)
ggplot(data = pPlot, aes(y = n, x = sum(n), fill = ethnicity)) +
  geom_bar(stat = "identity", position = "stack", color="black", alpha=0.5) +
  fill_palette("jco") +
  theme_pubr() +
  ylab("Count") +
  xlab("Ethnicity") +
  scale_x_discrete(labels = NULL)
```


### **7.4. BAR CHARTS - JAUNDICE / RELATIVE** {#bar-charts-jaundice-relative}

The bar chart on the left represents the relationship between a Jaundice diagnosis at birth, and being classed as having ASD. There is no significant relationship between the two present within this data-set. It is clear that the number of individuals diagnosed with Jaundice at birth is approximately the same for a classification of ASD = YES and ASD = NO.

The bar chart on the right represents the relationship between individuals that have a relative that has ASD and being classed as ASD themselves. Again, there does not appear to be any significant relationship between the two variables present in this data-set, as the number of individuals with relatives that have ASD is approximately the same for a classification of ASD = YES and ASD = NO. 

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 7.4. - Bar Charts - Jaundice / Relative*"}
# Plot relationships between jaundice, a relative with ASD and Class_ASD
p1 <- ggplot(data = dataASDclean, aes(x = Class_ASD, fill = jaundice)) +
  geom_bar(stat = "count", position = position_dodge(), color="black", alpha=0.5) +
  fill_palette("jco") +
  theme_pubr()

p2 <- ggplot(data = dataASDclean, aes(x = Class_ASD, fill = autism)) +
  geom_bar(stat = "count", position = position_dodge(), color="black", alpha=0.5) +
  fill_palette("jco") +
  theme_pubr()

require("gridExtra")
grid.arrange(p1, p2, ncol=2)
```


## **8. DATA PRE-PROCESSING** {#data-pre-processing}
***

### **8.1. VARIABLE REDUCTION** {#variable-reduction}

There are a number of variables present within the data-set that do not offer any benefit to our analysis.

* **Country of Residence:** In initial testing of the classification models, this variable did not display any significant importance to the accuracy of predictions. As a factor with more than 60 levels, it is too large to be processed by certain classification functions within the R environment. So, to ensure compatibility it will be removed.
* **Used App Before:** This variable denotes whether an individual used the screening application or not. It is not a significant indicator of our target variable, so this will also be removed.
* **Age Description:** This variable categories the age range of an individual. Everyone over the age of 17 is classed as an adult. As all of our observations are for adults aged 17 and older, this factor has only one level, therefore it can offer no significant benefit to the analysis.
* **Result:** As discussed previously in this report, a result value of 7 or more will always be classified as `Class_ASD` = YES. Therefore, including this variable would mean that the machine learning algorithms would essentially already have the outcome of the target variable. For the purposes of this analysis it will also be removed.

```{r echo = FALSE}
# Drop unnecessary variables (country_of_res, used_app_before, age_desc, and result)
dataASDclean <- dataASDclean[,-c(16, 17, 18, 19)]
```


Below are the remaining variables.

```{r echo = FALSE}
# Names of processed data
names(dataASDclean)
```


### **8.2. PARTITIONING** {#partitioning}

We will be partitioning the data-set into separate training and testing subsets. Two thirds of the data will be randomly sampled and allocated to the training set, and one third will be allocated to the testing set. While we will be using Repeated K-Fold Cross Validation to aid in choosing the most appropriate model, partitioning beforehand allows us to validate the model's accuracy by feeding it the unseen testing set.

The output below shows the number of observations present in each of the subsets.

```{r echo = FALSE}
# Partition the dataset (2/3 for training the model, 1/3 for testing)
set.seed(123)
split = sample.split(dataASDclean, SplitRatio = 2/3)
dataTrain = subset(dataASDclean, split == TRUE)
dataTest = subset(dataASDclean, split == FALSE)

Training_Observations <- paste(count(dataTrain))
Testing_Observations <- paste(count(dataTest))

ttObv <- cbind(Training_Observations, Testing_Observations)

pander(ttObv)
```


## **9. CONCEPTS & TECHNIQUES** {#concepts-techniques}
***

Here, we will breifly describe of the concepts and techniques used in the interpretation and evaluation of our classifcation results.


### **9.1 OVERFITTING vs. UNDERFITTING** {#overfitting-vs-underfitting}

Underfitting occurs when a model is too simple.  It has too few features or regularized too much which makes it inflexible in learning from the dataset. 

Overfitting is where the model works well on the trained data and gives a high accuracy rate but when presented with New Data it does not provide accurate results.  Too many factors can result in overfitting or noise.  When combined with bootstrapping and bagging we can find the optimum numbers of factors to use i.e. if graphed would create a peak.
In predictive modelling, you can think of the "signal" as the true underlying pattern that you wish to learn from the data.

"Noise," on the other hand, refers to the irrelevant information or randomness in a dataset.
Here's where machine learning comes in. A well-functioning ML algorithm will separate the signal from the noise.
If the algorithm is too complex or flexible (e.g. it has too many input features or it's not properly regularized), it can end up "memorizing the noise" instead of finding the signal. This overfit model will then make predictions based on that noise. It will perform unusually well on its training data but very poorly on new, unseen data.


### **9.2. K-FOLD CROSS VALIDATION** {#k-fold-cross-validation}

Generally, a classifier is decided from training data using a classifier learning algorithm. Each classifier has an associated prediction error, also called the true error. Usually, the true error is unknown, cannot be calculated, and must be estimated from data. This error is called estimated prediction error. (Juan Diego Rodriguez, Aritx Perez, and Jose Antonio Lozano, 2010). There are some estimators of the classification error such as Bootstrap, Resubstitution, Hold-out and more.

To measure the performance of classifiers used in this study, K-Fold Cross Validation was employed. In K-Fold Cross Validation, the data is partitioned into K-subsets. A portion of the data is reserved for testing(prediction) and the remaining data is used for training the model. (Daniel Bone, Matthew S. Goodwin, Matthew P. Black, Chi-Chun Lee, Kartik Audhkhasi, Chrikanth Narayanan, 2014). In this study, we have used a 10-Fold Cross Validation repeated 5 times on 2 different machine learning algorithms (Decision Tree and Random Forest). 10-Fold Cross Validation will use 90% of the entire data as a training set and 10% of the data as a test set and repeat this activity for 5 times.

The code below depicts the creation of a trainControl object using the "Caret" package. This object will be called when executing the train function on our training set and process the algorithm using repeated k-Fold cross valaidation.

```{r}
# Set up Repeated K-Fold Cross Validation
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
```


### **9.3. CONFUSION MATRIX** {#confusion-matrix}

The confusion matrix, also known as an error matrix, is a pre-processing of the data that describes how well a classification model is predicting its target variables. A confusion matrix shows the number of correct and incorrect predictions made by the classification model compared to the actual outcomes in the data, which means, on a classification problem, it is a brief description of the prediction results.

For Robert Susmaga (2016) the confusion matrix is very useful as it, "provides much more detailed information on the results of the test than the mere accuracy or error. It shows which classes were classified properly or almost properly and which where misclassified/confused with other classes and in what degree".

The performance is frequently evaluated using the data set in the matrix, allowing the visualization of the algorithms. The matrix is N by N where N the number of classes with predicted classes (output classes) and actual classes (target classes). Predictive models don't make assumptions, so it is very important that the performance is measured. Above the Confusion Matrix (Kohavi and Provost, 1998) shows the 2 classes classifier example:

```{r echo = FALSE}
confTable <- data.frame(
  Actual_Predicted <- c("Negative", "Positive"),
  Negative <- c("a", "b"),
  Positive <- c("c", "d"))

names(confTable) <- c("Actual_Predicted", "Negative", "Positive")

pander(confTable, justify = "left", style = "grid")
```

(Kohavi and Provost, 1998)

* **a:** The true negative rate (TN) is the number of correct negative predictions.
* **b:** The false positive rate (FP) is the number of incorrect positive predictions.
* **c:** The false negative rate (FN) is the number of incorrect negative predictions.
* **d:** The True positive rate or recall (TP) is the number of correct positive predictions.


### **9.4. ROC CURVE** {#roc-curve}

The receiver operating characteristic (ROC) curve, which is defined as a plot of test sensitivity as the y coordinate versus its 1-specificity or false positive rate (FPR) as the x coordinate, is an effective method of evaluating the performance of diagnostic tests. (Seong Ho Park, Jin Mo Goo, Chan-Hee Jo, 2004)

The area under the ROC curve(AUC), or the equivalent Gini index, is a widely used measure of performance of supervised classification rules. (DAVID J. HANDROBERT, 2001) This study also utilises ROC Curves and the area under the ROC curve to visualize the performance of our prediction models.


## **10. RESULTS** {#results}
***

### **10.1. DECISION TREE RESULTS** {#decision-tree-results}

```{r echo = FALSE, results = 'hide'}
# Grow decision tree based on cross validation criteria
set.seed(234)
fitdt <- train(Class_ASD ~., dataTrain, method = "rpart", trControl = trctrl)
```


Below is the output from the training phase of our Decision Tree model. Here, we can see that our repeated k-Fold cross validation has determined that the highest accuracy of approximately 87% occured with a Complexity Parameter (CP) of 0.0754717. We can now test the accuracy of the model's predictions using our testing set.

```{r echo = FALSE}
# Display results. ### Note ###  - Highest accuracy (87%) achieved with cp = 0.0754717
fitdt
```


This plot is a visual representation of the above information. It shows us that model accuracy drops as the complexity parameter increases.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 10.1. - Accuracy of Complexity Parameter (CP)*"}
# Plot fitdt
plot(fitdt)
```


Here, we can see the level of importance the model has assigned to the variables.

```{r echo = FALSE}
# Variable importance
impdt <- varImp(fitdt, scale = FALSE)
impdt
```


Plotting this information allows us to see that the model assigned the most significant importance to positive responses to the binary variables of:

* **A6:** I know how to tell if someone listening to me is getting bored?
* **A9:** I find it easy to work out what someone is thinking or feeling just by looking at their face?
* **A5:** I find it easy to read between the lines when someone is talking to me?

These three variables, which are questions answered during the ASD screening process, are all related to social interactions. We can infer from this that answering yes to these three questions is a strong indicator that an individual may be more likely to have ASD.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 10.2. - D-Tree - Variable Importance*"}
# Plot Varaiable Importance
plot(impdt)
```


And here we have the actual decision tree.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 10.3. - D-Tree*"}
# Plot the fitted decision tree
fancyRpartPlot(fitdt$finalModel, caption = "")
```


```{r echo = FALSE}
# Perform class prediction on the unseen test data using the constructed model
set.seed(345)
predASDdt <- predict(fitdt, newdata = dataTest)
```


To test the prediction accuracy of the model, We can now introduce new unseen data in the form of our testing data-set. Below is the confusion matrix output after performing predictions on the test set.

We have achieved 83% accuracy, which is very close to the accuracy statistic of 87% achieved during the training phase.

```{r echo = FALSE}
# Confusion Matrix of predicted values. ### Note ###  - Accuracy of 83%. Very close to our training result
confusionMatrix(table(predASDdt, dataTest$Class_ASD))
```


Plotting the confusion matrix, we can see the model's correct predictions in green, and incorrect predictions in red.

* The model correctly predicted a class of "NO" 126 times, and incorrectly predicted "NO" 21 times.
* The model correctly predicted a class of "YES" 53 times, and incorrectly predicted "YES" 15 times.

It would appear that this model is much more effective at predicting a class of "NO". However, this could be due to the nature of the data. There are more observations with a `Class_ASD` of "NO" than "YES" present in the data-set. Therefore it would be more difficlut to learn how to predict "YES" than "NO".

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 10.4. - D-Tree - Confusion Matrix*"}
# Plot decision tree confusion matrix
confusion_matrix <- table(predASDdt, dataTest$Class_ASD)
fourfoldplot(confusion_matrix, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Decision Tree Confusion Matrix")
```


The ROC curve visually represents the prediction performance of the model. For reference, a model that can predict with 100% accuracy would show a line that is a 90 degree angle, going all the way up to 1.0 sensitivity, and then a sharp right turn towards the diagonal line. Conversely, if the curve hugs the diagonal line, it's prediction accuracy is closer to 50%, or no better than random chance.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 10.5. - D-Tree - ROC Curve*"}
# Perform probability prediction on the test data
predASDProb1 <- predict(fitdt, newdata = dataTest, type = "prob")

# Plotting the ROC curve and calculating the AUC metric ### RESEARCH ###
auc <- auc(dataTest$Class_ASD, predASDProb1[,2])
plot(roc(dataTest$Class_ASD, predASDProb1[,2]), main = "Decision Tree ROC Curve")
```


### **10.2. RANDOM FOREST RESULTS** {#random-forest-results}


```{r echo = FALSE, results = 'hide'}
# Perfrom cross validated Random Forest on the training data
set.seed(456)
fitrf <- train(Class_ASD ~., dataTrain, method = "rf", trControl = trctrl)
```


Below is the output and plot from the training phase for a Random Forest model. Here, we can see that the repeated k-Fold cross validation has determined that the highest accuracy of approximately 95% occured with a mtry value of 14. We can now test the accuracy of this model's predictions using our testing set.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 10.6. - Accuracy of Randomly Selected Predictors*"}
# Display results. ### NOTE ### - Highest accuracy (95%) achieved with mtry = 14
fitrf
plot(fitrf)
```


Checking on the variable importance that the Random Forest model has assigned to the variables, we can see that, just like the Decision Tree model, it has selected positive responses to the `A9`, `A6` and `A5` as the three most important variables when classifying the target variable of `Class_ASD`. Although, it should be noted that the amount of importance attributed to each them is significantly different than the Decision Tree model. `A9` for example, appears to be much more important than all other variables.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 10.7. - R-Forest - Variable Importance*"}
# Plot variable importance
imprf <- varImp(fitrf, scale = FALSE)
imprf
plot(imprf)
```


```{r echo = FALSE}
# Perform class prediction on the test data
set.seed(567)
predASDrf <- predict(fitrf, newdata = dataTest)
```


Testing the trained Random Forest model on our test data-set, we can see that the model has predicted the target variable with a 93% accurcy when presented with unseen data.

```{r echo = FALSE}
# Confusion Matrix of predicted values. ### NOTE ### - Accuracy of 93%. Very close to our training result
confusionMatrix(table(predASDrf, dataTest$Class_ASD))
```


Plotting the confusion matrix, we can see the Random Forest model's correct predictions in green, and incorrect predictions in red.

* The model correctly predicted a class of "NO" 139 times, and incorrectly predicted "NO" just 13 times.
* The model correctly predicted a class of "YES" 61 times, and incorrectly predicted "YES" 2 times.

This appears to be a much more promising model than the Desicion tree. It displays stronger prediction accuracy for both the "YES" and "NO" classes of `Class_ASD`.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 10.8. - R-Forest - Confusion Matrix*"}
# Plot random forest confusion matrix
confusion_matrix <- table(predASDrf, dataTest$Class_ASD)
fourfoldplot(confusion_matrix, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Random Forest Confusion Matrix")
```


Plotting the ROC curve for the Random Forest model, we can see that the line comes a lot closer to the right angle shape mentioned previously. Compared to the Decision Tree model, this curve indicates a much stronger prediction accuracy.

```{r echo = FALSE, cache = TRUE, fig.align = 'center', fig.cap = "*fig 10.9. - Random Forest ROC Curve*"}
# Perform probability prediction on the test data
set.seed(678)
predASDProb2 <- predict(fitrf, newdata = dataTest, type = "prob")

# Plotting the ROC curve and calculating the AUC metric ### RESEARCH ###
auc <- auc(dataTest$Class_ASD, predASDProb2[,2])
plot(roc(dataTest$Class_ASD, predASDProb2[,2]), main = "Random Forest ROC Curve")
```


## **11. CONCLUSION** {#conclusion}
***

Our goal with this study was to apply supervised machine learning algorithms (Decsion Tree and Random Forest) to a data-set derived from Autism Spectrum Disorder research with the hopes of classify new observations into the categories of "Has ASD" or "Does Not Have ASD" (in the case of this study, these observations, would be new individuals that have undergone the ASD screening process).

We cleansed the data-set, by correcting outlier values, removing observations containg missing values and dropping unecessary variables. The loss of 95 observations is not ideal. However, it is difficult to replace missing values of a categorical nature as there is no median or mean to work with. While there are methods to impute missing categorical data, we decided that this course of action was beyond the scope of this project.

Using a repeated K-Fold cross validation, as well as a partition of our data into training and testing subsets, we were able to build two machine learning models and confirm that they are capable of predicting the target variable of Class_ASD with a high level of accuracy when given new data (Decision Tree: 83%, Random Forest: 93%).

While both models perform well, the Random Forest model is clearly superior in it's prediction accuracy. However, we discovered that both models are much more successful at predicting the negative responses to our target variable than they are at predicting the positive responses. We theorise that this is due to the smaller proportion of positive responses to Class_ASD being present within the data-set. Retraining the models on a larger, more balanced data-set is recommended to improve prediction accuracy.


## **12. REFERENCES** {#references}
***

Brown, M. (2012) 'Data Mining and techniques'. [Online]. IBM Developer Works. Available from: https://www.ibm.com/developerworks/library/ba-data-mining techniques/. [Accessed 16th April 2018].

Jain, S. (2016). 'Analysis and Application of Data Mining Methods used for Customer Churn in Telecom Industry'. [Online]. LinkedIn. Available from: https://www.linkedin.com/pulse/analysis-application-data-mining-methods-used-customer-saurabh-jain/ [Accessed: 16th April 2018]

Zhang, Z. (2016). 'Decision tree modelling using R'. [Online]. NCBI.  Available from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4980381/. [Accessed: 18th April 2018]

HSE (2017). 'Asperger syndrome (see Autistic spectrum disorders)'. [Online]. Health Service Executive. Available from: https://www.hse.ie/eng/health/az/a/asperger-syndrome/adults-living-with-autism.html. [Accessed: 18th April 2018]

Williams, G. (2010). 'Complexity (cp)'. [Online]. Togware. Available from: http://datamining.togaware.com/survivor/Complexity_cp.html. [Accessed 20th April 2018]

Witten, Ian. Eibe, Frank. A.Hall Mark. J.Pal, Christopher. Data Mining. Practical Machine Learning Tools and Techniques. Fourth Edition. https://books.google.ie/books?hl=en&lr=&id=1SylCgAAQBAJ&oi=fnd&pg=PP1&dq=classification+data+mining+technique&ots=8IBMtlixwd&sig=vofzsQKzaxzfSvvU88zgIUzDH80&redir_esc=y#v=onepage&q=classification%20concept&f=false. [Accessed 22nd April 2018]

Susmaga R. Confusion Matrix Visualization. (2004) In: Klopotek M.A., Wierzchon S.T., Trojanowski K. (eds) Intelligent Information Processing and Web Mining. Advances in Soft Computing, vol 25. Springer, Berlin, Heidelberg. https://link.springer.com/chapter/10.1007/978-3-540-39985-8_12 P1-2.  [Accessed 22nd April 2018]

Kohavi, R. and F. Provost (1998). Glossary of terms. Editorial for the Special Issue on Applications of Machine Learning and the Knowledge Discovery Process. http://robotics.stanford.edu/~ronnyk/glossary.html . [Accessed 22nd April 2018].

Gollapudi, S, & Laxmikanth, V 2016, Practical Machine Learning, Birmingham, UK: Packt Publishing, eBook Business Collection (EBSCOhost), EBSCOhost. [Accessed 22nd April 2018].

Brownlee, Jason (2016) Overfitting and Underfitting With Machine Learning Algorithms https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/ . [Accessed 22nd April 2018] 

Collen P.Chen, Christopher L.Keown, Afroos Jahedi, Aarti Nair, Mark E. Pflieger, Barbara, A.Bailey, Ralph-Axel Muller. (2015). Diagnostic classification of intrinsic functional connectivity highlights somatosensory, default mode, and visual regions in autism. NeuroImage: Clinical 8, 238-245. Retrieved from http://dx.doi.org/10.1016/j.nicl.2015.04.002 [Accessed 22nd April 2018] 

Daniel Bone, Matthew S. Goodwin, Matthew P. Black, Chi-Chun Lee, Kartik Audhkhasi, Chrikanth Narayanan. (2014). Applying Machine Learning to Facilitate Autism Diagnostics: Pitfalls and Promises. Springer Science + Business Media New York. 

Gotham, K., Risi, S., Pickles, A., & Lord, C. (2007). The autism diagnostic observation schedule: Revised algorithms for improved diagnostic validity. Journal of Autism and Developmental Disorders 37(4), 613-627.

Juan Diego Rodriguez, Aritx Perez, and Jose Antonio Lozano. (2010). Sensitivity Analysis of k-Fold Cross Validation in Prediction Error Estimation. IEEE TRANSACTIONS ON PATTER ANALYSIS AND MACHINE INTELLIGENCE, VOL.32, 569-575.

Lord, C., Risi, S., Lambrecht, L., Cook, E. H, Jr, Leventhal, B. L., DiLavore, P. C., et al. (2000). The Autism Diagnostic Observation Schedule - Generic: A standard measure of social and communication deficits associated with the spectrum of autism. Journal of Autism and Developmental Disorders, 30(3), 205-223. 

Seong Ho Park, MD,1 Jin Mo Goo, MD,1 and Chan-Hee Jo, PhD. (2004, March 31). KoreaMed. Retrieved from synapse.koreamed: https://synapse.koreamed.org/ [Accessed 22nd April 2018]

DAVID J. HAND, R. J. (2001). A Simple Generalisation of the Area Under the ROC. Machine Learning, 45, 171-186. 

